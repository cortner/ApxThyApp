{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MA3J8 Approximation Theory and Applications \n",
    "\n",
    "## 08b - Multivariate Approximation: Convergence Rates\n",
    "\n",
    "We explore in some very simple cases the effect of choosing subset of the full tensor product Chebyushev basis. Due to software and algorithmic limitations, these tests will be restricted to very low dimension, just 2 and 3. But we can still observe some of our analytic result and more importantly how they are extremely limited. The take-away message is that approximation in high dimension is extremely subtle and requires substantially more work than in 1D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: to run this notebook you need\n",
    "# add https://github.com/ettersi/ApproxTools.jl.git\n",
    "# but need the specific commit: \n",
    "#    eae0ae1bff2bddbb4f13824366227e490af42041\n",
    "# From the Julia repl it should be possible to install this with \n",
    "#  ] add https://github.com/ettersi/ApproxTools.jl.git#eae0ae1bff2bddbb4f13824366227e490af42041 \n",
    "# get in touch if this isn't working as expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ApproxTools, PyCall, Plots, PyPlot\n",
    "mplcolors = pyimport(\"matplotlib.colors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function greedy(p, M)\n",
    "    q = deepcopy(p)\n",
    "    I = sortperm(abs.(p.coeffs[:]), rev=true)\n",
    "    q.coeffs[I[M+1:end]] .= 0.0\n",
    "    return q \n",
    "end\n",
    "\n",
    "function sparsify(p, accfun)\n",
    "    q = deepcopy(p)\n",
    "    M = 0\n",
    "    for k in CartesianIndices(q.coeffs)\n",
    "        if accfun(Tuple(k))\n",
    "            M += 1\n",
    "        else \n",
    "            q.coeffs[k] .= 0.0\n",
    "        end \n",
    "    end\n",
    "    return q, M\n",
    "end \n",
    "\n",
    "sparsegrid(p, N) = sparsify(p, k -> (sum(k) <= N))\n",
    "\n",
    "hcross(p, N) = sparsify(p, k -> (prod(1 .+ k) <= N+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2(x1,x2) = exp(x1*sin(3*x2))\n",
    "\n",
    "NN = 2:2:30\n",
    "NNsp = 4:4:36\n",
    "NNhc = 5:12:200\n",
    "MM = [] \n",
    "MM_gr = 10:15:150\n",
    "MM_sp = []\n",
    "MM_hc = []\n",
    "err_ten = []\n",
    "err_gr = [] \n",
    "err_sp = [] \n",
    "err_hc = [] \n",
    "for N in NN \n",
    "    p_ten = approximate(f2, Chebyshev(N))\n",
    "    push!(err_ten, fnorm(f2, p_ten, 201))\n",
    "    push!(MM, length(p_ten.coeffs))\n",
    "end \n",
    "for M in MM_gr \n",
    "    p_ten = approximate(f2, Chebyshev(3*ceil(Int, sqrt(M))))\n",
    "    p_gr = greedy(p_ten, M)\n",
    "    push!(err_gr, fnorm(f2, p_gr, 201))\n",
    "end \n",
    "for N in NNsp \n",
    "    p_ten = approximate(f2, Chebyshev(N))\n",
    "    p_sp, M1 = sparsegrid(p_ten, N)\n",
    "    push!(err_sp, fnorm(f2, p_sp, 201))\n",
    "    push!(MM_sp, M1)\n",
    "end\n",
    "for N in NNhc \n",
    "    p_ten = approximate(f2, Chebyshev(N))\n",
    "    p_hc, M2 = hcross(p_ten, N)\n",
    "    push!(err_hc, fnorm(f2, p_hc, 201))\n",
    "    push!(MM_hc, M2)\n",
    "end\n",
    "\n",
    "\n",
    "Plots.plot(; yaxis = (:log,\"error\"), xlabel=\"#coeffs\" )\n",
    "plot!(MM, err_ten, lw=2, m=:o, ms=6, label=\"tensor\")\n",
    "plot!(MM_gr, err_gr, lw=2, m=:o, ms=6, label=\"greedy\")\n",
    "plot!(MM_sp, err_sp, lw=2, m=:o, ms=6, label =\"sparse\")\n",
    "plot!(MM_hc, err_hc, lw=2, m=:o, ms=6, label =\"hyp-x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f3(x1,x2,x3) = exp(x1*sin(3*x2)*x3^2)\n",
    "\n",
    "NN = 2:2:30\n",
    "NNsp = 4:4:44\n",
    "MM = [] \n",
    "MM_gr = [30, 60, 110, 190, 280, 400, 550, 750]\n",
    "MM_sp = []\n",
    "MM_hc = []\n",
    "err_ten = []\n",
    "err_gr = [] \n",
    "err_sp = [] \n",
    "err_hc = [] \n",
    "for N in NN \n",
    "    p_ten = approximate(f3, Chebyshev(N))\n",
    "    push!(err_ten, fnorm(f3, p_ten, 201))\n",
    "    push!(MM, length(p_ten.coeffs))\n",
    "end \n",
    "for M in MM_gr \n",
    "    p_ten = approximate(f3, Chebyshev(2*ceil(Int, sqrt(M))))\n",
    "    p_gr = greedy(p_ten, M)\n",
    "    push!(err_gr, fnorm(f3, p_gr, 201))\n",
    "end \n",
    "for N in NNsp \n",
    "    p_ten = approximate(f3, Chebyshev(N))\n",
    "    p_sp, M1 = sparsegrid(p_ten, N)\n",
    "    push!(err_sp, fnorm(f3, p_sp, 201))\n",
    "    push!(MM_sp, M1)\n",
    "end\n",
    "\n",
    "Plots.plot(; yaxis = (:log,\"error\"), xlabel=\"#coeffs\" )\n",
    "plot!(MM, err_ten, lw=2, m=:o, ms=6, label=\"tensor\")\n",
    "plot!(MM_gr, err_gr, lw=2, m=:o, ms=6, label=\"greedy\")\n",
    "plot!(MM_sp, err_sp, lw=2, m=:o, ms=6, label =\"sparse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can visualise the decay of the Chebyshev coefficients - at least in 2D:\n",
    "p = approximate(f2, Chebyshev(50)) \n",
    "imshow(1e-12 .+ abs.(coeffs(p)), norm=mplcolors.LogNorm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example: the multi-variate Runge function \n",
    "$$\n",
    "f(x) = \\frac{1}{1+c |x|^2},\n",
    "$$\n",
    "where $|x|$ is the 2-norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this time, let us start by visualising the chebyshev coefficients right away:\n",
    "fr2(x1, x2) = 1 / (1+10*(x1^2+x2^2))\n",
    "p = approximate(fr2, Chebyshev(100))\n",
    "imshow(1e-12 .+ abs.(coeffs(p)), norm=mplcolors.LogNorm())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clearly observe a radial decay of the Chebyshev coefficients! Thus suggests that we should truncate at $|k| \\leq N$ where $|k|$ is the 2-norm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = 10:10:100\n",
    "NNsp = 15:15:140\n",
    "NN2 = 10:10:100\n",
    "MM = [] \n",
    "MM_gr = 100:200:2000\n",
    "MM_sp = []\n",
    "MM_2 = []\n",
    "err_ten = []\n",
    "err_gr = [] \n",
    "err_sp = [] \n",
    "err_2 = [] \n",
    "for N in NN \n",
    "    p_ten = approximate(fr2, Chebyshev(N))\n",
    "    push!(err_ten, fnorm(fr2, p_ten, 201))\n",
    "    push!(MM, length(p_ten.coeffs))\n",
    "end \n",
    "for M in MM_gr \n",
    "    p_ten = approximate(fr2, Chebyshev(3*ceil(Int, sqrt(M))))\n",
    "    p_gr = greedy(p_ten, M)\n",
    "    push!(err_gr, fnorm(fr2, p_gr, 201))\n",
    "end \n",
    "for N in NNsp \n",
    "    p_ten = approximate(fr2, Chebyshev(N))\n",
    "    p_sp, M1 = sparsegrid(p_ten, N)\n",
    "    push!(err_sp, fnorm(fr2, p_sp, 201))\n",
    "    push!(MM_sp, M1)\n",
    "end\n",
    "for N in NN2\n",
    "    p_ten = approximate(fr2, Chebyshev(N))\n",
    "    p_hc, M = sparsify(p_ten, k -> sum(abs2, k) < N^2)\n",
    "    push!(err_2, fnorm(fr2, p_hc, 201))\n",
    "    push!(MM_2, M)\n",
    "end\n",
    "\n",
    "\n",
    "Plots.plot(; yaxis = (:log,\"error\"), xaxis=(:log,), xlabel=\"#coeffs\" )\n",
    "plot!(MM, err_ten, lw=2, m=:o, ms=6, label=\"tensor\")\n",
    "plot!(MM_gr, err_gr, lw=2, m=:o, ms=6, label=\"greedy\")\n",
    "plot!(MM_sp, err_sp, lw=2, m=:o, ms=6, label =\"sparse\")\n",
    "plot!(MM_2, err_2, lw=2, m=:o, ms=6, label =\"2-norm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the gain in the previous exampe comes from the factor \n",
    "due to ignoring the zero coefficients that arise due to symmetry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final example we consider a function that arises in transport theory. Specifically, we consider functions of the form \n",
    "$$\n",
    "  f(x_1, x_2) = \\frac{g(x_1, x_2)}{x_1 - x_2 + \\epsilon i},\n",
    "$$\n",
    "where $\\eta i$ is a small shift into the complex plane and $g$ is smooth. For small $\\epsilon$ $f$ has a singularity very close in the complex plane, or in fact an entire line of singularities and this significantly slows approximation. \n",
    "\n",
    "The function is then used to evaluate a bivariate matrix function, i.e., \n",
    "$$\n",
    "  f(H, H)\n",
    "$$\n",
    "where $H \\in \\mathbb{R}^{n \\times n}$ with $n$ potentially large. We have to be careful about how to interpret this function. A canonical definition is via tensor products. If $f(x_1, x_2) = f_1(x_1) f_2(x_2)$, then $f(H, H) = f_1(H) \\otimes f_2(H)$, which is \n",
    "a fours-dimensional tensor. Then, invoking linearity, if $f$ is a sum of tensor products, e.g., $f(x_1,x_2) = \\sum_k c_k T_{k_1}(x_1) T_{k_2}(x_2)$, we can write \n",
    "$$\n",
    "    f(H,H) = \\sum_k c_k T_{k_1}(H) \\otimes T_{k_2}(H).\n",
    "$$\n",
    "We may again employ the recursion formula for the Chebyshev basis to evaluate the basis. But even without going into the details it is clear that it will be crucial to minimise the number of terms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following picture shows significant potential for \n",
    "# sparsification!\n",
    "ft(x1,x2) = 1/(x1 - x2 + 0.01im)\n",
    "p = approximate(ft, Chebyshev(2000)) \n",
    "imshow(1e-12.+abs.(coeffs(p)), norm=mplcolors.LogNorm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is no sign here of a sparse grid or \n",
    "# hyperbolic cross like structure so we go straight for the \n",
    "# greedy approximation. This is justified in this case, \n",
    "# where the real cost is the evaluation!\n",
    "\n",
    "NN = 100:100:1000\n",
    "MM = [] \n",
    "MM_gr = 50:50:500\n",
    "err_ten = []\n",
    "err_gr = [] \n",
    "for N in NN \n",
    "    p_ten = approximate(ft, Chebyshev(N))\n",
    "    push!(err_ten, fnorm(ft, p_ten, 201))\n",
    "    push!(MM, length(p_ten.coeffs))\n",
    "end \n",
    "p_ten = approximate(fr2, Chebyshev(2000))\n",
    "for M in MM_gr \n",
    "    p_gr = greedy(p_ten, M)\n",
    "    push!(err_gr, fnorm(fr2, p_gr, 201))\n",
    "end \n",
    "\n",
    "Plots.plot(; yaxis = (:log,\"error\"), xaxis=(:log, \"#coeffs\") )\n",
    "plot!(MM, err_ten, lw=2, m=:o, ms=6, label=\"tensor\")\n",
    "plot!(MM_gr, err_gr, lw=2, m=:o, ms=6, label=\"greedy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
