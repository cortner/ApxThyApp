{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using Flux, Plots, Colors, IJulia\n",
    "using Base.Iterators: repeated\n",
    "gr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A univariate approximation problem  \n",
    "\n",
    "We try to train a DNN to approximate the fermi-dirac function. Why? Just because this has been one of our standard examples..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell sets up the problem, the data and the ANN network structure \n",
    "beta = 100\n",
    "trueFun(x) = [1/(1+exp(beta*x[1]))]\n",
    "\n",
    "# Here we are taking measurements from the true model\n",
    "# In real applications, we would just be given this \n",
    "# data rather than generate it ourselves\n",
    "samples = [ [x] for x in range(-1, 1, length=1_000) ]\n",
    "y = trueFun.(samples)\n",
    "\n",
    "# This defines an ANN with an input layer, \n",
    "# two hidden layers with 10 nodes each and an output \n",
    "# layer (try Flux.sigmoid?)\n",
    "nn = Chain(x -> x,                       # input layer\n",
    "           Dense(1, 10, tanh),   # first hidden layer\n",
    "           Dense(10, 10, tanh),  # second hidden layer \n",
    "           Dense(10, 10, tanh),  # third hidden layer \n",
    "           Dense(10, 1))                 # output layer\n",
    "\n",
    "\n",
    "# errors at individual sample points \n",
    "sqerrors() = [sum(abs2, z) for z in y - Flux.data(nn.(samples))]\n",
    "# loss functional => Least squares \n",
    "losssq() = sum(sqerrors())/length(samples)\n",
    "# for plotting the errors \n",
    "errcols() = [sqrt(e.data) for e in sqerrors()]\n",
    "\n",
    "# a call-back function to watch the optimisation...\n",
    "function cb() #callback function to observe training\n",
    "    # plot current prediction against data\n",
    "    xp = [ x[1] for x in samples ]\n",
    "    yx = [y[1] for y in y]\n",
    "    yp = [ Flux.data(y[1]) for y in nn.(samples) ]\n",
    "    IJulia.clear_output(true)\n",
    "    P = plot(xp, yx, lw=2, label=\"exact\")\n",
    "    plot!(P, xp, yp, lw=2, label=\"ANN\")\n",
    "    title!(P,  \"RMSE = $(sqrt(losssq().data))\")\n",
    "    display(P)\n",
    "end\n",
    "\n",
    "cb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the parameters: \n",
    "# the weights w^{[n]} and shifts b^{[n]}\n",
    "ps = Flux.params(nn)\n",
    "# and now we can watch the training \n",
    "Flux.train!(losssq, ps, repeated((), 2_000), ADAM(0.05), cb = cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: \"Learning\" a constitutive law\n",
    "Our first example is to model a constitutive law for an ODE\n",
    "$$\n",
    "   \\dot{u} = f(u)\n",
    "$$\n",
    "The exact law is \n",
    "$$\n",
    "  f({\\bf u}) = A {\\bf u}^3,\n",
    "$$\n",
    "where ${\\bf u} \\in \\mathbb{R}^2$, $A \\in \\mathbb{R}^{2 \\times 2}$ and ${\\bf u}^3$ means component-wise power.\n",
    "\n",
    "Initially we will assume that we are able to just take measurements of $f$, i.e., for various sample points ${\\bf u}$ we are able to evaluate $f({\\bf u})$. This is not entirely a realistic setting, but it can get us started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell sets up the problem, the data \n",
    "# and the ANN network structure \n",
    "# -----------------------------------------\n",
    "trueFun(u) = [-0.1 -0.1; -2.0 2.0] * u.^3\n",
    "\n",
    "# Here we are taking measurements from the true model\n",
    "# In real applications, we would just be given this \n",
    "# data rather than generate it ourselves\n",
    "nsamples = 100\n",
    "samples = [ rand(2) for n = 1:nsamples ]\n",
    "y = trueFun.(samples)\n",
    "\n",
    "# This defines an ANN with an input layer, \n",
    "# one hidden layer with 50 nodes and an output \n",
    "# layer   (try Flux.sigmoid?)\n",
    "nn = Chain(x -> x,                       # input layer\n",
    "           Dense(2, 10, Flux.sigmoid),   # first hidden layer\n",
    "           Dense(10, 10, Flux.sigmoid),  # second hidden layer \n",
    "           Dense(10, 2))                 # output layer\n",
    "\n",
    "\n",
    "# errors at individual sample points \n",
    "sqerrors() = [sum(abs2, z) for z in y - Flux.data(nn.(samples))]\n",
    "# loss functional => Least squares \n",
    "losssq() = sum(sqerrors())/length(samples)\n",
    "# for plotting the errors \n",
    "errcols() = [sqrt(e.data) for e in sqerrors()]\n",
    "\n",
    "# a call-back function to watch the optimisation...\n",
    "function cb() #callback function to observe training\n",
    "    # plot current prediction against data\n",
    "    x1 = [ x[1] for x in samples ]\n",
    "    x2 = [ x[2] for x in samples ]\n",
    "    err = errcols()\n",
    "    IJulia.clear_output(true)\n",
    "    display(scatter(x1, x2, zcolor=err, clims = (0.01, 0.1), label=\"\", \n",
    "                    title = \"RMSE = $(sqrt(losssq().data))\", \n",
    "                    size = (400,350)))\n",
    "end\n",
    "\n",
    "cb()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the parameters: \n",
    "# the weights w^{[n]} and shifts b^{[n]}\n",
    "ps = Flux.params(nn)\n",
    "# and now we can watch the training \n",
    "Flux.train!(losssq, ps, repeated((), 2_000), ADAM(0.05), cb = cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now produce contour plots of the two functions\n",
    "x = range(0, 1, length=100)\n",
    "X = [ [s, t]  for s in x for t in x]\n",
    "Ftrue1 = [f[1] for f in trueFun.(X)]\n",
    "Ftrue2 = [f[2] for f in trueFun.(X)]\n",
    "Fann1 = [f.data[1] for f in nn.(X)]\n",
    "Fann2 = [f.data[2] for f in nn.(X)]\n",
    "Ptrue1 = contour(x, x, Ftrue1, clim=[-0.2,0.0], levels=27)\n",
    "Ptrue2 = contour(x, x, Ftrue2, clim=[-0.2,0.0], levels=27)\n",
    "Pann1 = contour(x, x, Fann1, clim=[-2.0,2.0], levels=27)\n",
    "Pann2 = contour(x, x, Fann2, clim=[-2.0,2.0], levels=27)\n",
    "plot(Ptrue1, Pann1, Ptrue2, Pann2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our final example we return to the ODE problem. But now we don't assume that we can evaluate the constitutive law directly, but we are only allowed to observe the trajectories!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, DiffEqFlux, DifferentialEquations, Plots\n",
    "\n",
    "u0 = Float32[2.; 0.]\n",
    "datasize = 30\n",
    "tspan = (0.0f0,1.5f0)\n",
    "\n",
    "function trueODEfunc(du,u,p,t)\n",
    "    true_A = [-0.1 2.0; -2.0 -0.1]\n",
    "    du .= ((u.^3)'true_A)'\n",
    "end\n",
    "\n",
    "t = range(tspan[1],tspan[2],length=datasize)\n",
    "prob = ODEProblem(trueODEfunc,u0,tspan)\n",
    "u = solve(prob,Tsit5()) \n",
    "\n",
    "# but we are only allowed to observe u at certain times t: \n",
    "ode_data = Array(solve(prob, Tsit5(), saveat=t))\n",
    "\n",
    "plot(u)\n",
    "scatter!(t, ode_data[1,:], c=1, label = \"samples\")\n",
    "scatter!(t, ode_data[2,:], c=2, label = \"samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# specify the ANN: we are helping the ANN a bit \n",
    "# by giving the input layer some information about \n",
    "# the problem!\n",
    "dudt = Chain(x -> x.^3,          # input layer\n",
    "             Dense(2,50,tanh),   # one hidden layer\n",
    "             Dense(50,2))        # output layer\n",
    "\n",
    "ps = Flux.params(dudt)\n",
    "\n",
    "# specify the ode-solver using the ANN as the \n",
    "# driving force => to return the approximation solution \n",
    "# at the sample points \n",
    "n_ode = x->neural_ode(dudt, x, tspan, Tsit5(), saveat=t,\n",
    "                      reltol=1e-7, abstol=1e-9)\n",
    "\n",
    "function predict_n_ode()\n",
    "  n_ode(u0)\n",
    "end\n",
    "loss_n_ode() = sum(abs2, ode_data .- predict_n_ode())\n",
    "\n",
    "function cb() #callback function to observe training\n",
    "  # plot current prediction against data\n",
    "  cur_pred = Flux.data(predict_n_ode())\n",
    "  IJulia.clear_output(true)\n",
    "  pl = scatter(t,ode_data[1,:],c=1,label=\"data\", title=\"loss=$(loss_n_ode().data)\")\n",
    "  scatter!(t,ode_data[2,:],c=2,label=\"data\")\n",
    "  scatter!(pl,t,cur_pred[1,:],c=1,m=:star,label=\"prediction\")\n",
    "  scatter!(pl,t,cur_pred[2,:],c=2,m=:star,label=\"prediction\")\n",
    "  display(pl)\n",
    "end\n",
    "\n",
    "# Display the ODE with the initial parameter values.\n",
    "cb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.train!(loss_n_ode, ps, repeated((), 1000), ADAM(0.05), cb = cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Classification\n",
    "\n",
    "Our second class of examples are classification problem:  Given samples $y_m \\in \\{e_1, e_2\\}$ at spatial locations $x_m \\in [0,1]^2$ we are trying to find a function $y = f(x)$ such that $y_m \\approx f(x_m)$. Note that it is common to use unit vectors $e_i$ to describe the different categories. For general points $x$, $f(x)$ will be a vector with mixed coordinates, which is information that we can use to give a confidence to the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some curve that separates the data \n",
    "ff(x) = 0.5*(exp(-3*(x[1]-0.5)) - 1)^2+0.25\n",
    "f(x) = x[2] - ff(x[1])\n",
    "nsamples = 200 \n",
    "samples = [ rand(2) for n = 1:nsamples ]\n",
    "IA = findall(f.(samples) .> 0) \n",
    "IB = findall(f.(samples) .<= 0) \n",
    "x1 = [s[1] for s in samples]\n",
    "x2 = [s[2] for s in samples]\n",
    "xx = range(0, 1, length=200)\n",
    "scatter(x1[IA], x2[IA], label=\"A\")\n",
    "scatter!(x1[IB], x2[IB], label=\"B\")\n",
    "plot!(xx, ff.(xx), lw=3, label=\"boundary\", ylims=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the sample data \n",
    "y = [ zeros(2) for _=1:nsamples ]\n",
    "for n in IA; y[n] = [1.0,0.0]; end \n",
    "for n in IB; y[n] = [0.0,1.0]; end \n",
    "\n",
    "# defines ANN \n",
    "nn = Chain(x -> x,             # input layer\n",
    "           Dense(2,10,Flux.sigmoid),   # first hidden layer\n",
    "           Dense(10,10,Flux.sigmoid),   # second hidden layer\n",
    "           Dense(10,2))        # second NN layer\n",
    "\n",
    "# extract the parameters: \n",
    "# the weights w^{[n]} and shifts b^{[n]}\n",
    "ps = Flux.params(nn)\n",
    "\n",
    "# errors at individual sample points \n",
    "sqerrors() = [sum(abs2, z) for z in y - Flux.data(nn.(samples))]\n",
    "# loss functional => Least squares \n",
    "losssq() = sum(sqerrors())/length(samples)\n",
    "# for plotting the errors \n",
    "errcols() = [sqrt(e.data) for e in sqerrors()]\n",
    "\n",
    "# a call-back function to watch the optimisation...\n",
    "function cb() #callback function to observe training\n",
    "    println(\"RMSE = $(sqrt(losssq().data))\")\n",
    "    # plot current prediction against data\n",
    "    err = errcols()\n",
    "    IJulia.clear_output(true)\n",
    "    display(scatter(x1, x2, zcolor=err, clims = (0.033, 0.33), label=\"\"))\n",
    "end\n",
    "\n",
    "cb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.train!(losssq, ps, repeated((), 1_000), ADAM(0.05), cb = cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a test set \n",
    "nvalidate = 500\n",
    "samples = [ rand(2) for n = 1:nvalidate ]\n",
    "x1 = [s[1] for s in samples]\n",
    "x2 = [s[2] for s in samples]\n",
    "IA = findall(f.(samples) .> 0) \n",
    "IB = findall(f.(samples) .<= 0) \n",
    "y_nn = [y.data for y in nn.(samples)]\n",
    "IA_nn = findall( [(abs(y[1]) > abs(y[2])) for y in y_nn] )\n",
    "IB_nn = setdiff(1:length(y_nn), IA_nn)\n",
    "plot(xx, ff.(xx), label=\"boundary\", c=3, lw=3, ylims=[0,1])\n",
    "scatter!(x1[IA], x2[IA], c=1, label=\"A\")\n",
    "scatter!(x1[IB], x2[IB], c=2, label=\"B\")\n",
    "scatter!(x1[IA_nn], x2[IA_nn], m=:cross, c = 1, label=\"A_nn\")\n",
    "scatter!(x1[IB_nn], x2[IB_nn], m=:cross, c = 2, label=\"A_nn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Handwriting Recognition\n",
    "\n",
    "As our final example we consider hand-writing recognition,\n",
    "another fairly elementary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, Flux.Data.MNIST, Statistics\n",
    "using Flux: onehotbatch, onecold, crossentropy, throttle\n",
    "using Base.Iterators: repeated\n",
    "\n",
    "# load images from MNIST database \n",
    "imgs = MNIST.images()\n",
    "# and the corresponding classification\n",
    "labels = MNIST.labels()\n",
    "# show the first four images \n",
    "display(hcat(imgs[1:4]...))\n",
    "println(labels[1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack images into one large batch\n",
    "# for faster processing...\n",
    "X = hcat(float.(reshape.(imgs, :))...)\n",
    "Y = onehotbatch(labels, 0:9)\n",
    "\n",
    "# ANN : this network architecture is classical construction\n",
    "#       I'm not sure why this is such a good choice?!\n",
    "m = Chain(  # 28 x 28 pixel image as input\n",
    "  Dense(28^2, 32, relu),  # hidden layer with max(0,x) activation \n",
    "  Dense(32, 10),          # output layer => 10 different digits\n",
    "  softmax)                # apply soft-max to output to filter\n",
    "\n",
    "# look up cross entropy loss functions\n",
    "# in the documentation / any ANN tutorial\n",
    "loss(x, y) = crossentropy(m(x), y)\n",
    "\n",
    "# how many images are correctly classified?\n",
    "accuracy(x, y) = mean(onecold(m(x)) .== onecold(y))\n",
    "\n",
    "# a naive prediction implementation\n",
    "predict(img) = findmax(m(float.(reshape(img, :))).data)[2]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass in the whole dataset 200 times, i.e., \n",
    "# 200 training steps / optimisation steps \n",
    "dataset = repeated((X, Y), 200)\n",
    "# call-back to show progress \n",
    "evalcb = () -> @show(loss(X, Y))\n",
    "Flux.train!(loss, params(m), dataset, ADAM(), \n",
    "            cb = throttle(evalcb, 10))  # output progress only every 10 iterations\n",
    "# display the accuracy of the fit \n",
    "accuracy(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Images\n",
    "# test set \n",
    "MNIST.images(:test)[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy on the test set is comparable with the training set!\n",
    "tX = hcat(float.(reshape.(MNIST.images(:test), :))...)\n",
    "tY = onehotbatch(MNIST.labels(:test), 0:9)\n",
    "accuracy(tX, tY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's try to load some handwriting and see how we do!\n",
    "# Convert to grayscale at loading \n",
    "using FileIO, Images \n",
    "imgs = [ Gray.(1 .- Gray.(load(\"digits/$n.png\"))) for n = 0:9 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the predictions!\n",
    "for n = 0:9 \n",
    "    println(\"n = $n => \", predict(imgs[n+1]))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
