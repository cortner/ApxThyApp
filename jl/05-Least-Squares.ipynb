{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MA3J8 Approximation Theory and Applications\n",
    "\n",
    "## 05 - Least Squares Methods\n",
    "\n",
    "In this notebooks we will explore in a little more detail how to fit functions using least squares fitting instead of interpolation. This is closer to the Hilbert-space setting, i.e., best approximation by projection.\n",
    "\n",
    "First we implement some auxiliary functions: evaluating the chebyshev basis functions as well as a polynomial represented in terms of those basis functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots, Polynomials, QuadGK, SoftGlobalScope, LinearAlgebra, LaTeXStrings, Plots, Printf\n",
    "gr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "A convenience wrapper to store a polynomial in terms \n",
    "of its coefficients and its basis\n",
    "\"\"\"\n",
    "struct Poly\n",
    "    c::Vector\n",
    "    evalb\n",
    "end\n",
    "(p::Poly)(x) = dot(p.c, p.evalb(x, length(p.c)-1))\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "linear lsq fit using the QR factorisation\n",
    "f : function to be fitted \n",
    "X : sample points \n",
    "N : polynomial degree \n",
    "evalbasis : a function to evaluate the basis at a point x \n",
    "\"\"\"\n",
    "function pfit(f, X, N, evalbasis)\n",
    "    Y = f.(X)\n",
    "    A = lsqsys(X, N, evalbasis)\n",
    "    c = qr(A) \\ Y\n",
    "    return Poly(c, evalbasis)\n",
    "end\n",
    "\n",
    "\n",
    "function lsqsys(X, N, evalbasis)\n",
    "    M = length(X)\n",
    "    A = zeros(M, N+1)\n",
    "    for m = 1:M\n",
    "        A[m, :] = evalbasis(X[m], N)\n",
    "    end \n",
    "    return A \n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "evaluate the monomial basis at a point \n",
    "\"\"\" \n",
    "monobasis(x, N) = [x^n for n = 0:N]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05 - 1 Motivation / Fit via QR Factorisation\n",
    "\n",
    "We begin by recalling a naive result from our polynomial approximation tests, where we experimented using least squares in place of (equi-spaced) polynomial interpolation. But this time, let's use our own code and also push the polynomial degree a bit higher!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = 1 / (1 + 25 * x^2)\n",
    "NN = 10:10:100\n",
    "X = range(-1, 1, length=1000)\n",
    "\n",
    "P = plot(X, f.(X), lw=2, label = \"f\")\n",
    "ylims!(P, (-0.05, 0.2))\n",
    "\n",
    "err = []\n",
    "for N in NN\n",
    "    p = pfit(f, X, N, monobasis)\n",
    "    push!(err, norm(f.(X) - p.(X), Inf))\n",
    "    \n",
    "    # TODO: add N = 100 to the plot!\n",
    "    if N in [10, 20, 40, 100]; plot!(P, X, p.(X), lw=2, label = \"p$N\"); end \n",
    "end \n",
    "\n",
    "plot(P, plot(NN, err, lw=2, m=:o, yaxis = (:log,), label = \"\"), layout = (1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that around $N = 40$ even $M = 400$ datapoints are not enough to produce a stable / accurate fit? Or does this have to do with the conditioning of the basis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\" degree |  cond(A)   |  norm(c)\")\n",
    "println(\"--------|------------|----------\")\n",
    "for N in NN[1:end-1] \n",
    "    p = pfit(f, X, N, monobasis)\n",
    "    cnd = cond(lsqsys(X, N, monobasis))\n",
    "    @printf(\"   %3d  |  %.2e  |  %.2e \\n\", N, cnd, norm(p.c))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us attempt the same using the Chebyshev basis. We know this is a much better behaved basis. The results now look much more promising!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "evaluate the chebyshev basis at a point x \n",
    "\"\"\" \n",
    "function chebbasis(x, N)\n",
    "    @assert N >= 2 \n",
    "    T = zeros(typeof(x), N+1)\n",
    "    T[1] = 1\n",
    "    T[2] = x \n",
    "    for n = 2:N \n",
    "        T[n+1] = 2*x*T[n] - T[n-1]\n",
    "    end \n",
    "    return T \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = 1 / (1 + 100 * x^2)\n",
    "NN = 10:30:300\n",
    "xp = range(-1, 1, length=1000)\n",
    "errmono = [] \n",
    "errcheb = []\n",
    "for N in NN\n",
    "    pm = pfit(f, xp, N, monobasis)\n",
    "    push!(errmono, norm(f.(xp) - pm.(xp), Inf))\n",
    "\n",
    "    pc = pfit(f, xp, N, chebbasis)\n",
    "    push!(errcheb, norm(f.(xp) - pc.(xp), Inf))\n",
    "end \n",
    "plot(NN, errmono, lw=2, m=:o, yaxis = (:log,), label = \"monofit\")\n",
    "plot!(NN, errcheb, lw=2, m=:o, label = \"chebfit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\" degree |  cond(A)   |  norm(c)\")\n",
    "println(\"--------|------------|----------\")\n",
    "for N in NN[1:end-1] \n",
    "    pc = pfit(f, xp, N, chebbasis)\n",
    "    cnd = cond(lsqsys(xp, N, chebbasis))\n",
    "    @printf(\"   %3d  |  %.2e  |  %.2e \\n\", N, cnd, norm(pc.c))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we have made a very naive mistake ... we are estimating the error on the same points on which we are training! Studying this issue in detail goes far beyond this module, but we can at least test it numerically by estimating the error on a much finer grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = 1 / (1 + 100 * x^2)\n",
    "NN = 10:30:300\n",
    "x_train = range(-1, 1, length=1000)\n",
    "x_test = range(-1, 1, length=4*1000)\n",
    "errmono1 = [] \n",
    "errcheb1 = []\n",
    "for N in NN\n",
    "    pm = pfit(f, x_train, N, monobasis)\n",
    "    push!(errmono1, norm(f.(x_test) - pm.(x_test), Inf))\n",
    "\n",
    "    pc = pfit(f, x_train, N, chebbasis)\n",
    "    push!(errcheb1, norm(f.(x_test) - pc.(x_test), Inf))\n",
    "end \n",
    "plot(NN, errmono, lw=2, m=:o, yaxis = (:log,), label = \"polyfit-train\")\n",
    "plot!(NN, errcheb, lw=2, m=:o, label = \"chebfit-train\")\n",
    "plot!(NN, errmono1, lw=2, m=:o, yaxis = (:log,), label = \"polyfit-test\")\n",
    "plot!(NN, errcheb1, lw=2, m=:o, label = \"chebfit-test\")\n",
    "ylims!((1e-7, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be related to conditioning of the normal equations! See the table above! We can see that this is a form of overfitting related to the Runge phenomenon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1 = plot(x_train, f.(x_train), lw=2, label = \"f\")\n",
    "for N in [100, 200, 400]\n",
    "    pc = pfit(f, x_train, N, chebbasis)\n",
    "    plot!(x_test, pc.(x_test), lw=1, label = \"cheb($N)\")\n",
    "end\n",
    "ylims!(-0.1, 0.3)\n",
    "\n",
    "x_plot = range(0.99, 0.999999, length=1_000)\n",
    "P2 = plot(x_plot, f.(x_plot), lw=2, label = \"\")\n",
    "pc = nothing \n",
    "for N in [100, 200, 400]\n",
    "    pc = pfit(f, x_train, N, chebbasis)\n",
    "    plot!(x_plot, pc.(x_plot), lw=2, label = \"\")\n",
    "end\n",
    "plot!(x_train, pc.(x_train), lw=0, c=4, m=:o, label  =\"\")\n",
    "xlims!(P2, (0.99, 1.005))\n",
    "ylims!(P2, (-0.12, 0.12))\n",
    "\n",
    "plot(P1, P2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05-2 Fitting with the right distribution \n",
    "\n",
    "In the previous section we did something that is actually quite odd. We used a Chebyshev basis to fit polynomials with datapoints $x_m$ uniformaly distributed in $[-1,1]$ whereas Chebyshev polynomials are orthogonal w.r.t the measure $(1-x^2)^{-1/2} dx$. It therefore seems natural to (1) either incorporate the Chebyshev weights \n",
    "$$\n",
    "w_m = (1 - x_m^2)^{-1/2}\n",
    "$$\n",
    "into the fitting process; or (2) to fit on Chebyshev-distributed data points.\n",
    "$$\n",
    "x_m = \\cos\\big(\\pi m/M\\big)\n",
    "$$\n",
    "We will next explore the consequences of this modification.\n",
    "\n",
    "NOTE: This can be done much more effectively using Gauss type quadrature rules, but this is not the point of these experiments which are gearing up towards the next section below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "function wchebfit(f, N, M, data=:unif, weights=:unif)\n",
    "    if data == :unif\n",
    "        X = range(-1, 1, length=M)\n",
    "    elseif data == :cheb\n",
    "        X = cos.(range(0, pi, length=M))\n",
    "    else\n",
    "        error(\"Unknown `data`\")\n",
    "    end\n",
    "    \n",
    "    if weights == :unif\n",
    "        W = ones(length(X))\n",
    "    elseif weights == :cheb  # W<-âˆšW\n",
    "        W = (1+1e-10 .- X.^2).^(-0.25) \n",
    "    else \n",
    "        error(\"Unknown `weights`\")\n",
    "    end\n",
    "        \n",
    "    # weighted lsq system\n",
    "    Y = W .* f.(X)\n",
    "    A = Diagonal(W) * lsqsys(X, N, chebbasis)\n",
    "    return Poly(qr(A) \\ Y, chebbasis)\n",
    "end \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easy parameters \n",
    "# C, Mfit, NN = 25, 400, 19:20:399\n",
    "# medium parameters \n",
    "C, Mfit, NN = 100, 400, 20:20:380\n",
    "# harder parameters \n",
    "# C, Mfit, NN = 400, 1_000, 20:40:400\n",
    "\n",
    "f(x) = 1 / (1 + C * x^2)\n",
    "x_test = cos.(range(0, pi, length=4000))\n",
    "\n",
    "err_uu, err_uc, err_cu = [], [], []\n",
    "get_err = (_N, _x, _w) -> norm(f.(x_train) - wchebfit(f, _N, Mfit, _x, _w).(x_train), Inf)\n",
    "\n",
    "for N in NN\n",
    "    push!(err_uu, get_err(N, :unif, :unif))\n",
    "    push!(err_uc, get_err(N, :unif, :cheb))\n",
    "    push!(err_cu, get_err(N, :cheb, :unif))\n",
    "end \n",
    "plot(; yaxis = (:log, ), xaxis = (\"polynomial degree\",), title = \"Max-Error\")        \n",
    "plot!(NN, err_uu, lw=2, m=:o, label = \"unif, unif\")\n",
    "plot!(NN, err_uc, lw=2, m=:o, label = \"unif, cheb\")\n",
    "plot!(NN, err_cu, lw=2, m=:o, label = \"cheb, unif\")\n",
    "plot!(NN, 0.1*(1+1/sqrt(C)).^(- NN), lw=2, c=:black, ls=:dash, label=\"predicted\")\n",
    "P_maxe = hline!([1e-16], label = \"eps\")\n",
    "plot(P_maxe, legend=:bottomleft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Conditioning of Chebyshev LSQ System\")\n",
    "println(\" degree |  cond(A)   |  norm(c)\")\n",
    "println(\"--------|------------|----------\")\n",
    "for N in 40:40:400\n",
    "    X = cos.(range(0, pi, length=1_000))\n",
    "    pc = pfit(f, X, N, chebbasis)\n",
    "    cnd = cond(lsqsys(X, N, chebbasis))\n",
    "    @printf(\"   %3d  |  %.2e  |  %.2e \\n\", N, cnd, norm(pc.c))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05-3 Fit at Random Points \n",
    "\n",
    "The most realistic real-world context is that we cannot choose the data-points but are given - likely random - datapoints. The first step should then be to construct an orthogonal basis adapted to the distribution of the points. Since we work mostly with Chebyshev here we assume that the distribution is the Chebyshev distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function rndchebfit(f, N, M, data)\n",
    "    if data == :unif\n",
    "        X = range(-1, 1, length=M)\n",
    "    elseif data == :cheb\n",
    "        X = cos.(range(0, pi, length=M))\n",
    "    elseif data == :rand\n",
    "        X = 2*(rand(M) .- 0.5)\n",
    "    elseif data == :randcheb\n",
    "        X = cos.(pi * rand(M))\n",
    "    else\n",
    "        error(\"Unknown `data`\")\n",
    "    end\n",
    "    \n",
    "    # weighted lsq system\n",
    "    p = pfit(f, X, N, chebbasis)\n",
    "    \n",
    "    # errors\n",
    "    X_test = cos.(range(0, pi, length=5*M))\n",
    "    return p, norm(f.(X_test) - p.(X_test), Inf)    \n",
    "end \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easy parameters \n",
    "C, Mfit, NN = 25, 400, 19:20:399\n",
    "# medium parameters \n",
    "# C, Mfit, NN = 100, 400, 20:20:380\n",
    "# harder parameters \n",
    "# C, Mfit, NN = 400, 981, 20:60:980\n",
    "\n",
    "\n",
    "f(x) = 1 / (1 + C * x^2)\n",
    "err_u, err_ru, err_c, err_rc = [], [], [], []\n",
    "\n",
    "for N in NN\n",
    "    push!(err_u, rndchebfit(f, N, Mfit, :unif)[2])\n",
    "    push!(err_ru, rndchebfit(f, N, Mfit, :rand)[2])\n",
    "    push!(err_c, rndchebfit(f, N, Mfit, :cheb)[2])\n",
    "    push!(err_rc, rndchebfit(f, N, Mfit, :randcheb)[2])\n",
    "end \n",
    "\n",
    "plot(; yaxis = (:log,), xaxis = (\"polynomial degree\",), title=\"RMSE-TEST\")\n",
    "plot!(NN, err_u, lw=2, m=:o, label = \"unif\")\n",
    "plot!(NN, err_ru, lw=2, ls=:dash, m=:o, label = \"unif, rnd\")\n",
    "plot!(NN, err_c, lw=2, m=:o, label = \"cheb\")\n",
    "plot!(NN, err_rc, lw=2, ls=:dash, m=:o, label = \"cheb, rnd\")\n",
    "P_maxe = hline!([1e-16], label = \"\")\n",
    "\n",
    "plot(P_maxe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis of [Cohen, Davenport, Leviatan, 2012] suggests that we should turn the problem around: given a number of data-points $M$ we should then choose an appropriate degree $N$. In practise it seems best to combine theory with experimentation. \n",
    "\n",
    "Here, we learn that we need $N$ slightly smaller than $M$ (a log-factor is suggested in the paper) to ensure that the LSQ system is stable (moderate condition number) with high probability. The additional error that arises is \n",
    "$$\n",
    "  M^{-r}\n",
    "$$\n",
    "where $M$ is the number of data points and $r$ is given by a complicated relation, but for the Chebyshev basis with random points $x_m$ distributed according to the Chebyshev distribtion we have $r \\approx C M / N$. To balance the errors we want $r \\log M \\approx \\alpha N$, i.e., $C M/N \\approx C \\log M$, or, $N \\approx C M/\\log M$. The $C$ is a bit difficult to determine analytically, so we do it experimentally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C, MM = 25, 50:50:1000\n",
    "f(x) = 1 / (1 + C * x^2)\n",
    "\n",
    "Nfun(M, Nsugg) = floor(Int, min(M-1, Nsugg))\n",
    "N1(M) = Nfun(M, M / log(M))\n",
    "N2(M) = Nfun(M, 3*M/log(M))\n",
    "N3(M) = Nfun(M, 5*M/log(M))\n",
    "N4(M) = Nfun(M, 7*M/log(M))\n",
    "\n",
    "err, err1, err2, err3, err4 = [], [], [], [], []\n",
    "for M in MM\n",
    "    push!(err,  rndchebfit(f, M-1, M, :cheb)[2])\n",
    "    push!(err1, rndchebfit(f, N1(M), M, :randcheb)[2])\n",
    "    push!(err2, rndchebfit(f, N2(M), M, :randcheb)[2])\n",
    "    push!(err3, rndchebfit(f, N3(M), M, :randcheb)[2])\n",
    "    push!(err4, rndchebfit(f, N4(M), M, :randcheb)[2])\n",
    "end \n",
    "\n",
    "plot(; yaxis = (:log,), xaxis = (\"polynomial degree\",), title=\"max-error\")\n",
    "plot!(MM.-1, err, lw=2, m=:o, label = \"chebyshev grid\")\n",
    "plot!(N1.(MM), err1, lw=2, m=:o, label = \"N= M/log(M)\")\n",
    "plot!(N2.(MM), err2, lw=2, m=:o, label = \"N=3M/log(M)\")\n",
    "plot!(N3.(MM), err3, lw=2, m=:o, label = \"N=5M/log(M)\")\n",
    "plot!(N4.(MM), err4, lw=2, m=:o, label = \"N=7M/log(M)\")\n",
    "\n",
    "PN = hline!([1e-16], label = \"eps\")\n",
    "\n",
    "plot(; yaxis = (:log,), xaxis = (\"# sample points\",), title=\"max-error\")\n",
    "plot!(MM, err, lw=2, m=:o, label = \"\")\n",
    "plot!(MM, err1, lw=2, m=:o, label = \"\")\n",
    "plot!(MM, err2, lw=2, m=:o, label = \"\")\n",
    "plot!(MM, err3, lw=2, m=:o, label = \"\")\n",
    "plot!(MM, err4, lw=2, m=:o, label = \"\")\n",
    "PM = hline!([1e-16], label = \"\")\n",
    "\n",
    "plot(PN, PM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
