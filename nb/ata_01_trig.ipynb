{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"tools.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# §1 Approximation with Trigonometric Polynomials\n",
    "\n",
    "## §1.1 Introduction\n",
    "\n",
    "Trigonometric polynomials are functions of the form\n",
    "$$\n",
    "t_N(x) = \\sum_{k = -N}^N c_k e^{i k x}\n",
    "$$\n",
    "Alternatively, we may write this as \n",
    "$$\n",
    "\tt_N(x) = a_0 + \\sum_{k = 1}^N \\Big( a_k \\cos(k x) + b_k \\sin(k x) \\Big),\n",
    "$$\n",
    "but we will always use the much more convenient complex exponential notation. \n",
    "\n",
    "Note that $t_N$ will be $2\\pi$-periodic, i.e. we are now restricting ourselves to approximating periodic functions. But in return we gain a cleaner and simpler theory. Further, everything we do applies to some extent also to algebraic polynomials.\n",
    "\n",
    "$N$ is called the degree of $t_N$. The space of all trigonometric polynomials of degree $N$ is denoted by \n",
    "$$\n",
    "\\mathcal{T}_N := {\\rm span}\\big\\{ x \\mapsto \\exp(i k x ) \\,|\\, k  = -N, \\dots, N \\big\\}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember we said that approximation theory is about approximating complex functions with simple functions. Are trigonometric polynomials \"simple\" ? In other words, is $e^{i k x}$ straightforward to implement? If $z = e^{i x}$ is available then we can obtain $e^{i k x} = z^k$ just by multiplication. Implementing $e^{i x}$ is not difficult either, e.g., we can use a truncated Taylor series; see the exercise 1.1.1. But there are much more efficient implementations using rational functions. We will not pursue this but just accept that somebody has done that job for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function classes\n",
    "\n",
    "The widest possible class of target functions $f$ we will be concerned with are continuous functions, i.e., functions belonging to the space \n",
    "$$\n",
    "  C_{\\rm per} := \\big\\{ f : \\mathbb{R} \\to \\mathbb{R} \\,|\\, \\text{ continuous, and } 2\\pi\\text{-periodic} \\big\\}.\n",
    "$$\n",
    "Technically we could go to classes of integrable functions, $L^p$, but this leads to further technicalities that less interesting.\n",
    "\n",
    "We will assume higher regularity later on, such as differentiability of certain order, analyticity, etc. But for now it is only important for us to know that all $f$ we encounter have well-defined point values. \n",
    "\n",
    "The natural norm on $C_{\\rm per}$ is the max-norm (or, sup-norm, or $L^\\infty$-norm), \n",
    "$$\n",
    "    \\| f \\|_\\infty := \\max_{x \\in [-\\pi, \\pi]} |f(x)|.\n",
    "$$\n",
    "This is also a natural measure for the approximation error: throughout this course we will focus on understanding approximation errors in this norm, i.e., given a function $f \\in C_{\\rm per}$ we aim to construct an approximation $t_N \\in \\mathcal{T}_N$ such that \n",
    "$$\n",
    "   \\|f - t_N \\|_\\infty \n",
    "$$\n",
    "is small, and to understand the rate of this error as $N \\to \\infty$. \n",
    "\n",
    "Another very natural measure of the error is the $L^2$-norm, $\\|f\\|_2 := (\\int_{-\\pi}^\\pi |f|^2 \\,dx)^{1/2}$, which we will explore in §3 on least squares methods. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## § 1.2 Trigonometric Interpolation\n",
    "\n",
    "A general degree $N$ trigonometric polynomial, \n",
    "$$\n",
    "   t_N(x) = \\sum_{k = -N}^N c_k e^{i k x}\n",
    "$$\n",
    "has $2N+1$ parameters $c_{-N}, \\dots, c_N$. How should we determine these? It would be ideal if we can prescribe exactly $2N+1$ conditions. A natural and general approach is *interpolation*: given a target function $f \\in C_{\\rm per}$ we demand that \n",
    "$$ \n",
    "\tt_N(x_j) = f(x_j), \\qquad j = 0, \\dots, 2N\n",
    "$$\n",
    "where $x_j$ are called the interpolation nodes. \n",
    "\n",
    "How should they be chosen? It turns out  that equi-spaced nodes work very well. An intuitive justification for this choice is that in a periodic domain, all parts of the domain are \"equal\" and should be treated the same. By contrast in a finite interval one should *not* use equispaced nodes, but rather cluster them at the boundary (cf. Chebyshev interpolation [LN, Ch. 4]). Thus, we may choose the interpolation nodes\n",
    "$$\n",
    "\tx_j = \\frac{\\pi j}{N}, \\qquad j = 0, \\dots, 2N.\n",
    "$$\n",
    "(Note we could equally use equispaced notes on $[-\\pi, \\pi]$ which often feels more natural, but the convention for certain algorithms -- see below -- is to use the nodes defined above.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the periodic boundary condition the nodes $x_0 = 0, x_{2N} = 2\\pi$ are \"equivalent\", in the sense that $f(x_0) = f(x_{2N})$, which is clearly a bad idea! A possibly way forward is to use instead the nodes $\\frac{2\\pi j}{2N+1}, j= 1, \\dots, 2N+1$. This might work (I haven't actually tested it!) but for algorithmic reasons it turns out that a much more convenient decision is to keep the nodes we defined above, but change the space of polynomials.\n",
    "\n",
    "Namely, consider the two basis functions $e^{iNx}$ and $e^{-iNx}$. What values do they take on the grid? \n",
    "$$\n",
    "   e^{i N x_j} = e^{i N j \\pi/N}  = e^{i\\pi j} \n",
    "  = (-1)^j = (-1)^{-j} = e^{-i \\pi j} = e^{- i N \\pi j/N} = e^{- i N x_j}.\n",
    "$$\n",
    "That is, these two basis functions are identical on the grid. In other words we have found the kernel of the interpolation operator on the equispaced grid. There are multiple possible solution, but a simple one is to replace these basis functions with their mean, i.e. \n",
    "$$\n",
    "\t\\frac{e^{i N x} + e^{-i N x}}{2} = \\cos(N x),\n",
    "$$\n",
    "which of course takes again the same values on the grid. This results in a modified trigonometric polynomial space which contains the range of the interpolation operator, \n",
    "$$\n",
    "\t\\mathcal{T}_N' := {\\rm span} \\Big( \\mathcal{T}_{N-1} \\cup \\{ \\cos(Nx) \\} \\Big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem:** The nodal interpolation operator $I_N : C_{\\rm per} \\to \\mathcal{T}_N'$ is well-defined, i.e., for all $f \\in C_{\\rm per}$ there exists a unique $I_N f := t_N \\in \\mathcal{T}_N$ such that \n",
    "$$\n",
    "\tt_N(x_j) = f(x_j) \\qquad \\text{for } j = 1, \\dots, 2N.\n",
    "$$\n",
    "\n",
    "**Proof:** Straightforward exercise, e.g., a simple way to prove it is that it can be reduced to algebraic polynomial interpolation in $\\mathbb{C}$. It will also follow from results we will cover later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation Notes\n",
    "\n",
    "* To define the nodal interpolation operator we setup a linear system that specifies the coefficients. If $I_N f(x) = \\sum_{k = -N+1}^{N-1} \\hat{F}_k e^{i k x} + \\hat{F}_N \\cos(Nx)$ then \n",
    "$$\n",
    "\t\\sum_{k = -N+1}^N \\hat{F}_k e^{i k x_j} = F_j \\qquad \\text{for } j = 0, \\dots, 2N-1,\n",
    "$$\n",
    "where $F_j = f(x_j)$ and $\\hat{F}_k$ are the coefficients. Knowing the coefficients we can then evaluate the interpolant.\n",
    "* Since $e^{i N x} = \\cos(Nx)$ on the interpolation nodes, it doesn't matter which of the two we use to construct the interpolation operator.\n",
    "* The ordering of the basis is in principle arbitrary. Here we use a convention used for fast algorithms (FFT), \n",
    "$$\n",
    "\t(0, 1, \\dots, N, -N+1, -N+1, \\dots, -1)\n",
    "$$\n",
    "This may look strange at first, but see more on this below!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"interpolation nodes\"\n",
    "xgrid(N) = range(0, 2*π - π/N, length = 2*N)\n",
    "\n",
    "\"fourier coefficient indices\"\n",
    "kgrid(N) = [0:N; -N+1:-1]\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let N = 3\n",
    "    # implement the nodal interpolation operator \n",
    "    A = [ exp(im * k * x) for k in kgrid(N), x in xgrid(N) ]\n",
    "    # observe that A'A ~ diagonal\n",
    "    real.(round.(A' * A, digits=12))\n",
    "    # confirm it is an orthogonal matrix (up to scaling)! I.e. (A'/2N) = inv(A) !!\n",
    "    # norm(A' * A - 2*N*I)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to guaranteeing that $I_N$ is well-defined we also see that the matrix $A$ we need to invert is orthogonal (up to rescaling), which makes it very easy to invert it. We just need to multiply by $A^H$, i.e. $O(N^2)$ computational cost instead of $O(N^3)$ for solving a full linear system via [Gaussian elimination](https://en.wikipedia.org/wiki/LU_decomposition).\n",
    "\n",
    "These two operations $F \\mapsto \\hat{F}$ and $\\hat{F} \\mapsto F$ are called the discrete and inverse discrete fourier transforms. They can in fact be applied with $O(N \\log(N))$ computational cost, using the *fast fourier transform*. We will study this later but for now use our naive implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "construct the coefficients of the trigonometric interpolant\n",
    "\"\"\"\n",
    "function triginterp(f, N)\n",
    "    X = xgrid(N)\n",
    "    # nodal values at interpolation nodes\n",
    "    F = f.(X) \n",
    "    # system matrix\n",
    "    A = [ exp(im * x * k) for k in kgrid(N), x in X ]\n",
    "    # coefficients are given by F̂ = A' * F as discussed above!\n",
    "    return (A' * F) / (2*N)\n",
    "end \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "to evaluate a trigonometric polynomial just sum coefficients * basis\n",
    "we the take the real part because we assume the function we are \n",
    "approximating is real.\n",
    "\"\"\"\n",
    "evaltrig(x, F̂) = sum( real(F̂k * exp(im * x * k))\n",
    "                      for (F̂k, k) in zip(F̂, kgrid(length(F̂) ÷ 2)) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to study some examples. Here is a selection of different functions we may want to approximate:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f_1(x) &= \\sin(x) \\\\\n",
    "f_2(x) &= \\sin(4x) \\\\\n",
    "f_3(x) &= |\\sin(2x)| \\\\\n",
    "f_4(x) &= |\\sin(2x)|^3 \\\\\n",
    "f_5(x) &= \\exp(\\cos(x)) \\\\\n",
    "f_6(x) &= \\frac{1}{1 + \\sin(x)^2} \\\\\n",
    "f_7(x) &= \\frac{1}{1 + 10*\\sin(x)^2} \\\\\n",
    "f_8(x) &= \\chi_{|x| < 2.5}  \\exp\\bigg(3 - \\frac{3}{1-(x/2.5)^2}\\bigg)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1(x) = sin(x)\n",
    "f2(x) = sin(4*x)\n",
    "f3(x) = abs(sin(x))\n",
    "f4(x) = abs(sin(x))^3\n",
    "f5(x) = exp(cos(x))\n",
    "f6(x) = 1 / (1 + sin(x)^2)\n",
    "f7(x) = 1 / (1.0 + 10*sin(x)^2)\n",
    "g8(x) = x^2 < 2.49999^2 ? exp(3 - 3 / (1-(x/2.5)^2)) : 0.0\n",
    "f8(x) = g8(mod(x+π,2*π)-π)\n",
    "\n",
    "allf = [f1, f2, f3, f4, f5, f6, f7, f8]\n",
    "flabels = latexstring.([\"f_$n\" for n in 1:8])\n",
    "\n",
    "xx = range(0, 2π, length=500)\n",
    "Ps = [ plot(xx, f.(xx), label = flabels[n], lw=2) \n",
    "       for (n, f) in enumerate(allf) ]\n",
    "plot(Ps..., layout = (4,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can just visually play around a bit with different target functions and degrees....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play with the functions and with the parameters! \n",
    "# if `Interact.jl` doesn't work for you, then replace the \n",
    "# @manipulate line with the let line and manually change \n",
    "# the parameters. \n",
    "# let fidx = 5, N = 7\n",
    "@manipulate for fidx = 1:8, N = 3:30\n",
    "    f = allf[fidx]\n",
    "    X = xgrid(N)\n",
    "    xp = range(0, 2π, length=200)\n",
    "    F̂ = triginterp(f, N)\n",
    "    plot(xp, f.(xp), lw=6, label = latexstring(\"f_{$fidx}\"), size = (450, 250))\n",
    "    plot!(xp, evaltrig.(xp, Ref(F̂)), lw=3, label = latexstring(\"I_{$N} f\"))\n",
    "    plot!(X, f.(X), lw=0, m=:o, ms=4, c = 2, label = \"\", \n",
    "         legend = :outertopright)\n",
    "    title!(latexstring(\"\\\\mathrm{Trigonometric~Interpolant:} f_$fidx, N = $N\"))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "....but this is not very informative. We need a more disciplined approach to measuring accuracy and evaluating the quality of the approximation scheme.\n",
    "\n",
    "We will now study how the errors $\\| f - t_N \\|_\\infty$ behave asymptotically as $N \\to \\infty$. This asymptotic behavious is called the *convergence rate*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "implementation of a basic error function \n",
    "\"\"\"\n",
    "function triginterperror(f, N; Nerr = 1_362)\n",
    "    xerr = range(0, 2π, length=Nerr)    # compute error on this grid\n",
    "    F̂ = triginterp(f, N)                # trigonometric interpolant\n",
    "    return norm(f.(xerr) - evaltrig.(xerr, Ref(F̂)), Inf)  # max-error on xerr grid\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let NN = [1:2:5; (2).^(3:7)], flist = allf, flabels = flabels\n",
    "    P = plot( xaxis  = (L\"N~{\\rm (degree)}\", ),\n",
    "              yaxis  = (:log, L\"\\Vert f - I_N f ~\\Vert_{\\infty}\"), \n",
    "              legend = :outertopright, \n",
    "              xscale = :log10, yscale = :log10, \n",
    "              size = (500, 300))\n",
    "    for (f, lab) in zip(flist, flabels)\n",
    "        err = triginterperror.(f, NN)\n",
    "        plot!(P, NN, err, lw=2, m=:o, ms=3, label = lab)\n",
    "    end\n",
    "    hline!(P, [1e-15], c = :red, lw=3, label = L\"\\epsilon\")\n",
    "end \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Straight lines on a log-log scale correspond to an *algebraic rate of convergence* \n",
    "and the slope tells us what the rate is:\n",
    "$$\n",
    "    \\log({\\rm err}(N)) \\sim p \\log(N)\n",
    "    \\qquad \\Leftrightarrow  \\qquad \n",
    "    {\\rm err}(N) \\sim N^p\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let NN = (2).^(2:7), flist = allf[[3,4]], flabels = flabels[[3,4]]\n",
    "    P = plot( xaxis  = (L\"N~{\\rm (degree)}\", ),\n",
    "              yaxis  = (:log, L\"\\Vert f - I_N f ~\\Vert_{\\infty}\"), \n",
    "              legend = :outertopright, \n",
    "              xscale = :log10, yscale = :log10, \n",
    "              size = (400, 250))\n",
    "    for (f, lab) in zip(flist, flabels)\n",
    "        err = triginterperror.(f, NN)\n",
    "        plot!(P, NN, err, lw=2, m=:o, ms=3, label = lab)\n",
    "    end\n",
    "    P\n",
    "end \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sublinear behaviour on a log-log plot means super-algebraic convergence. There are many examples of that kind, but the simplest (and typical) is exponentioal convergence, \n",
    "$$\n",
    "    {\\rm err}(N) \\sim e^{- \\alpha N}\n",
    "    \\qquad \\Leftrightarrow \\qquad \n",
    "    \\log({\\rm err}(N)) \\sim - \\alpha N\n",
    "$$\n",
    "i.e. the \"rate\" $\\alpha$ can be seen on a lin-log plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let NN = 4:4:40, flist = allf[5:8], flabels = flabels[5:8]\n",
    "    # check f8 by switching to NN = 10:10:100\n",
    "    P = plot( xaxis  = (L\"N~{\\rm (degree)}\", ),\n",
    "              yaxis  = (:log, L\"\\Vert f - I_N f ~\\Vert_{\\infty}\"), \n",
    "              legend = :outertopright, \n",
    "              yscale = :log10, \n",
    "              size = (400, 250))\n",
    "    for (f, lab) in zip(flist, flabels)\n",
    "        err = triginterperror.(f, NN)\n",
    "        plot!(P, NN, err, lw=2, m=:o, ms=3, label = lab)\n",
    "    end\n",
    "    P\n",
    "end "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## § 1.4 Approximation Error Estimates\n",
    "\n",
    "We have seen how to construct trigonometric polynomial approximants and have observed several different kinds of convergence behaviour. Our last goal for the first set of lectures is to explain these observations rigorously. This material is contained in my lecture notes (with further references to the literature). I won't reproduce it here in much detail, but only summarize the main results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### § 1.4.1 Best approximation and near-best approximation\n",
    "\n",
    "We will momentarily formulate several results in terms of *best approximation*, i.e., we will show results along the lines \n",
    "$$\n",
    "   \\inf_{t_N \\in \\mathcal{T}_N} \\|f - t_N \\|_\\infty \\leq \\epsilon_N,\n",
    "$$\n",
    "where $\\epsilon_N$ is some rate of convergence. Or equivalently, *there exist $t_N \\in \\mathcal{T}_N$ such that* $\\| f - t_N\\|_\\infty \\leq \\epsilon_N$. But it will not be clear from those results how to construct such best or quasi-best approximants.\n",
    "\n",
    "Here, a general principle comes into play that saves us: Let $I_N f$ denote the degree-$N$ trigonometric interpolant of $f$, then one can prove that \n",
    "$$\n",
    "\t\\| I_N f \\|_\\infty \\leq \\Lambda_N \\| f \\|_\\infty \\qquad \\forall f \\in C_{\\rm per}\n",
    "$$\n",
    "The constant $\\Lambda_N$ is called the Lebesgue constant and can be precisely estimated:\n",
    "\n",
    "**Theorem:** $\\Lambda_N \\leq \\frac{2}{\\pi} \\log(N+1) + 1$.\n",
    "\n",
    "**Proof:** See [LN, Thm 3.26] for a proof of a slightly weaker result. The proof is not difficult but quite dry, involving delicate estimates of the Dirichlet kernel. Please study this for further reading.\n",
    "\n",
    "In practise we will just write $\\Lambda_N \\lesssim \\log N$. With this in hand we have the following argument at our disposal: Let $t_N \\in \\mathcal{T}_N'$ be *any* trigonometric polynomial, then $I_N t_N = t_N$, and hence, \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t\\|f - I_N f \\|_\\infty \n",
    "\t&\\leq\n",
    "\t\\|f - t_N\\|_\\infty + \\| t_N - I_N f \\|_\\infty \\\\ \n",
    "    &= \\|f - t_N\\|_\\infty + \\| I_N (t_N - f) \\|_\\infty \\\\ \n",
    "\t&\\leq (1 + \\Lambda_N) \\|f - t_N \\|_\\infty.\n",
    "\\end{aligned}\n",
    "$$\n",
    "Now, taking the infimum over all $t_N \\in \\mathcal{T}_N'$ we obtain that \n",
    "$$\n",
    "   \\|f - I_N f \\|_\\infty \\leq (1+\\Lambda_N) \\inf_{t_N \\in \\mathcal{T}_N'} \\|f - t_N \\|_\\infty.\n",
    "$$\n",
    "Thus, the interpolation error is within a $\\log N$ factor of the best approximation error. In practise this cannot be observed, and indeed in some scenarios this factor can indeed be removed entirely. Because it is so mild, we won't worry about it. \n",
    "\n",
    "**Remark:** The foregoing argument is extremely general. The only property that we used here is that the interpolation operator $I_N : C_{\\rm per} \\to \\mathcal{T}_N'$ is a bounded operator with respect to the $\\|\\cdot\\|_\\infty$ norm. It is therefore easy to generalize and indeed used extensively throughout numerical analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### § 1.4.2 Jackson Theorems [LN, Ch. 3]\n",
    "\n",
    "A function $f \\in C_{\\rm per}$ has modulus of continuity $\\omega : \\mathbb{R} \\to \\mathbb{R}$ if \n",
    "$$\n",
    "|f(x) - f(x')| \\leq \\omega(|x - x'|) \\qquad \\forall x, x' \\in \\mathbb{R}.\n",
    "$$\n",
    "\n",
    "**First Jackson Theorem:** If $f \\in C_{\\rm per}$ has modulus of continuity $\\omega$, then there exists $t_N \\in \\mathcal{T}_N$ such that \n",
    "$$\n",
    "\t\\| f - t_N \\|_\\infty \\omega(N^{-1}).\n",
    "$$\n",
    "\n",
    "The second Jackson theorem extends the result to functions that are continuously differentiable. We say that $f \\in C^p_{\\rm per}$ if $f \\in C_{\\rm per}$, is $p$ times continuously differentiable on $\\mathbb{R}$ and all derivatives up to $f^{(p)} \\in C_{\\rm per}$. \n",
    "\n",
    "**Second Jackson Theorem:** Assume that $f \\in C_{\\rm per}^p$ and $f^{(p)}$ has modulus of continuity $\\omega$, then there exists $t_N \\in \\mathcal{T}_N$ such that \n",
    "$$\n",
    "\t\\| f - t_N \\|_\\infty N^{-p} \\omega(N^{-1}).\n",
    "$$\n",
    "\n",
    "A few comments and special cases:\n",
    "* The smoother $f$ the faster the convergence.\n",
    "* These results are qualitatively sharp, i.e. Jackson's construction has the optimal rate for these function classes.\n",
    "* If $f$ is Lipschitz then $\\omega(r) = L r$ where $L$ is the Lipschitz constant. An example of a Lipschitz continuous function is $f(x) = |\\sin(x)|$. \n",
    "* If $f$ is $\\sigma$-Hölder then $\\omega(r) = C r^\\sigma$. An example of a Hölder continuous function is $f(x) = |\\sin(x)|^\\sigma$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now explain the first set of numerical results: \n",
    "* $f_3$ is Lipschitz continuous, hence the rate is $N^{-1}$\n",
    "* $f_4 \\in C^2$ with $f_4''$ Lischitz hence the rate is $N^{-3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let NN = (2).^(2:7), flist = allf[[3,4]], flabels = flabels[[3,4]]\n",
    "    P = plot( xaxis  = (L\"N~{\\rm (degree)}\", ),\n",
    "              yaxis  = (:log, L\"\\Vert f - I_N f ~\\Vert_{\\infty}\"), \n",
    "              legend = :outertopright, \n",
    "              xscale = :log10, yscale = :log10, \n",
    "              size = (400, 250))\n",
    "    for (f, lab) in zip(flist, flabels)\n",
    "        err = triginterperror.(f, NN)\n",
    "        plot!(P, NN, err, lw=2, m=:o, ms=3, label = lab)\n",
    "    end\n",
    "    nn = NN[3:end]\n",
    "    plot!(nn, 1.3*nn.^(-1), lw=2, c=:black, ls=:dash, label = L\"N^{-1}, N^{-3}\")\n",
    "    plot!(nn, 3*nn.^(-3), lw=2, c = :black, ls=:dash, label = \"\")\n",
    "    P\n",
    "end "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also extend our tests to go beyond just the Lipschitz case and see if we can reproduce other kinds of convergence behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let \n",
    "#     f(x) = \n",
    "#     rate(N) = \n",
    "#     xp = range(-pi, pi, length=200)\n",
    "#     P1 = plot(xp, f.(xp); lw=3, label = \"f(x)\")\n",
    "    \n",
    "#     NN = (2).^(2:7)\n",
    "#     err = triginterperror.(f, NN)\n",
    "#     P2 = plot( NN, err, lw=2, m=:o, ms=3, label = \"error\", \n",
    "#                xaxis  = (L\"N~{\\rm (degree)}\", ),\n",
    "#                yaxis  = (:log, L\"\\Vert f - I_N f ~\\Vert_{\\infty}\"), \n",
    "#                legend = :topright, \n",
    "#                xscale = :log10, yscale = :log10, )\n",
    "\n",
    "#     nn = NN[3:end]\n",
    "#     plot!(P2, nn, 1.3*rate.(nn), lw=2, c=:black, ls=:dash, \n",
    "#           label = \"predicted\")\n",
    "\n",
    "#     plot(P1, P2, size = (550, 300))\n",
    "# end "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### § 1.4.3 Paley Wiener Theorems [LN, Sec. 2.3 and 3.4]\n",
    "\n",
    "First we review some standard results of complex analysis that we cannot reproduce here in detail. They will be covered in any introductory textbook or lecture notes on complex analysis.\n",
    "\n",
    "A function $f : \\mathbb{R} \\to \\mathbb{R}$ is called analytic in a point $x_0$ if it is represented by a Taylor series, i.e., \n",
    "$$\n",
    "\tf(x) = \\sum_{n = 0}^\\infty c_n (x - x_0)^n\n",
    "$$\n",
    "and the series has radius of convergence $r > 0$. If $f \\in C_{\\rm per}$ is analytic in $[-\\pi, \\pi]$ (and hence in all of $\\mathbb{R}$) then using the Taylor series one can extend it to a function on a strip in the complex plane: Let \n",
    "$$\n",
    "\\Omega_\\alpha := \\{ z \\in \\mathbb{C} : |{\\rm Im} z| < \\alpha \\}\n",
    "$$\n",
    "then there exists $\\alpha > 0$ and unique analytic $f : \\Omega_\\alpha \\to \\mathbb{C}$ which agrees with the original $f$ in $\\mathbb{R}$. \n",
    "\n",
    "**Payley Wiener Theorem:** Let $f \\in C_{\\rm per}$ have an analytic extension to $\\bar{\\Omega}_\\alpha$, then \n",
    "$$\n",
    "\tf(x) = \\sum_{k \\in \\mathbb{Z}} \\hat{f}_k e^{i k x} \n",
    "\t\\qquad \\text{with} \\qquad |\\hat{f}_k| \\leq M_\\alpha e^{- \\alpha |k|}\n",
    "$$\n",
    "where $M_\\alpha = \\sup_{z \\in \\bar{\\Omega}_\\alpha} |f(z)|$.\n",
    "\n",
    "**Corollary:**  Let $f \\in C_{\\rm per}$ have an analytic extension to $\\bar{\\Omega}_\\alpha$, then there exists $t_N \\in \\mathcal{T}_N$ such that \n",
    "$$\n",
    "\t\\|f - t_N\\|_\\infty \\leq M_\\alpha e^{-\\alpha N}\n",
    "$$\n",
    "\n",
    "The most important question for us is \"what is the maximal extension\"? I.e., what is the largest value $\\alpha$ such that $f$ can be extended to an analytic function in $\\Omega_\\alpha$? We will study this on the two examples $f_6, f_7$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to explain the remaining numerical results: \n",
    "\n",
    "* $f_5$ is entire, i.e., the error is bounded by $C_\\alpha e^{-\\alpha N}$ for all $\\alpha > 0$. This is called *superexponential* convergence. \n",
    "\n",
    "* $f_8 \\in C^\\infty$ but is not analytic. The point where it becomes zero has a Taylor expansion that evaluates to zero and hence has a zero convergence radius.\n",
    "\n",
    "* $f_6, f_7$ are both analytic. To determine the region of analyticity of a function $(1+c^2 \\sin^2(x))^{-1}$ we simply need to find where it has a pole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this slider to adjust the parameter $c \\in [1, 20]$ in the target function $1 / (1+ c \\sin^2(z))$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let c = 1.0\n",
    "@manipulate for c = 1:20\n",
    "    f = z -> 1 / (1 + c * sin(z)^2)\n",
    "    xp = range(-π, π, length=200)\n",
    "    yp = range(-2, 2, length=200)\n",
    "    contourf(xp, yp, (x,y) -> log(abs(f(x + im * y))), \n",
    "            xlabel = L\"x = Re(z)\", ylabel = L\"y = Im(z)\", size = (500, 400),\n",
    "            title = latexstring(\"|(1+ $c \\\\sin^2(z))^{-1}|\"), \n",
    "            levels=30) \n",
    "            # levels=-6.0:0.3:4.1)\n",
    "    hline!([0.0, asinh(1/sqrt(c)), - asinh(1/sqrt(c))], lw=2, c=:red, label = L\"\\mathbb{R}, \\pm \\alpha\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To carry this out rigorously we need to solve $c^2 \\sin^2(z) = -1$, where $c = 1$ for $f_6$ and $c = \\sqrt{10}$ for $f_7$. Thus we need to solve\n",
    "$$\n",
    "\t\\sin(z) = \\pm \\frac{i}{c}\n",
    "$$\n",
    "This is not difficult but needs a bit of complex analysis. With a bit of work one can prove that the poles occur at \n",
    "$$\n",
    "\tz = n \\pi \\pm i  \\sinh^{-1}(1/c) \\sim n\\pi \\pm i \\frac{1}{c} \\qquad \\text{as } c \\to \\infty.\n",
    "$$\n",
    "This gives us the *open* region of analyticity $\\Omega_\\alpha$ with $\\alpha = \\sinh^{-1}(1/c)$. To apply the Paley Wiener theorem we need to make $\\alpha$ just a little bit smaller, and we get \n",
    "$$\n",
    "\t\\| f - t_N \\|_\\infty \\leq C_\\epsilon e^{ - (\\alpha - \\epsilon) N } \\qquad \\forall \\epsilon > 0.\n",
    "$$\n",
    "But in practise we just ignore this $\\epsilon$ correction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let NN = 6:6:60, flist = allf[6:7], flabels = flabels[6:7]\n",
    "    P = plot( xaxis  = (L\"N~{\\rm (degree)}\", ),\n",
    "              yaxis  = (:log, L\"\\Vert f - I_N f ~\\Vert_{\\infty}\"), \n",
    "              legend = :outertopright, \n",
    "              yscale = :log10, \n",
    "              size = (450, 250))\n",
    "    for (f, lab) in zip(flist, flabels)\n",
    "        err = triginterperror.(f, NN)\n",
    "        plot!(P, NN, err, lw=2, m=:o, ms=3, label = lab)\n",
    "    end\n",
    "    nn1 = [20, 35]; nn2 = [30, 50]\n",
    "    plot!(nn1, 10*exp.( - asinh(1.0) * nn1), lw=2, ls=:dash, c=:black, label = L\"\\exp(-\\alpha_i N)\")\n",
    "    plot!(nn2, 10*exp.( - asinh(1/sqrt(10.0)) * nn2), lw=2, ls=:dash, c=:black, label = \"\")\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let \n",
    "#     f(x) = real(sqrt(0.1 + im * sin(x)))\n",
    "#     rate(N) = \n",
    "#     xp = range(-pi, pi, length=200)\n",
    "#     P1 = plot(xp, f.(xp); lw=3, label = \"f(x)\")\n",
    "    \n",
    "#     NN = 20:20:200\n",
    "#     err = triginterperror.(f, NN)\n",
    "#     P2 = plot( NN, err, lw=2, m=:o, ms=3, label = \"error\", \n",
    "#                xaxis  = (L\"N~{\\rm (degree)}\", ),\n",
    "#                yaxis  = (:log, L\"\\Vert f - I_N f ~\\Vert_{\\infty}\"), \n",
    "#                legend = :topright, yscale = :log10, )\n",
    "\n",
    "#     nn = NN[4:end]\n",
    "#     plot!(P2, nn, rate.(nn), lw=2, c=:black, ls=:dash, \n",
    "#           label = \"predicted\")\n",
    "\n",
    "#     plot(P1, P2, size = (550, 300))\n",
    "# end "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes our first set computational experiments: \n",
    "* trigonometric polynomials\n",
    "* interpolation with trigonometric polynomials\n",
    "* rates of convergence \n",
    "* implementation and numerical investigation of convergence rates\n",
    "* summary of rigorous results (Jackson, Paley-Wiener)\n",
    "* matching theory and numerical results\n",
    "\n",
    "The next topics will be \n",
    "* fast evaluation of the trigonometric interpolant via the fast fourier transform\n",
    "* numerical solution of BVPs and and IBVPs using trigonometric polynomials (aka spectral methods)\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "92bba00ff4204a9a83754f4a6a1014d6",
   "lastKernelId": "b46b3479-a3f6-4665-bcd4-ceaf8fae0130"
  },
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
